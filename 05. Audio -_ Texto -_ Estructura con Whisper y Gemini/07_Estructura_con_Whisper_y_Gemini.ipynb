{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Audio - ASR (Automatic Speech Recognition)\n",
        "\n",
        "Reconocimiento automático del habla (ASR), también conocido como texto a voz (STT), es la tarea de transcribir un audio dado a texto. Tiene muchas aplicaciones, pero nosotros nos vamos a enfocar en la tarea de extraer informacion y generar datasets con este tipo de modelos."
      ],
      "metadata": {
        "id": "vcx6mbR_O9xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper\n",
        "\n",
        "Whisper es un modelo de reconocimiento de voz de propósito general. Está entrenado en un gran conjunto de datos de audio diverso y también es un modelo multitarea que puede realizar reconocimiento de voz multilingüe, traducción de voz y identificación de idioma.\n",
        "\n",
        "podemos trabajar con whisper directamente desde la libreria de [transformers](https://huggingface.co/docs/transformers/model_doc/whisper), o con [openai-whisper](https://github.com/openai/whisper). en este caso vamos a usar la libreria trasnformers.\n"
      ],
      "metadata": {
        "id": "kqSaleuAPTc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPlkC1PnOay0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers accelerate\n",
        "!pip install  tiktoken pandas openai requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio de Ejemplo\n",
        "\n",
        "[Simulacro de audiencia ante Juzgado Cívico de Tonalá, Jalisco](https://www.youtube.com/watch?v=soxHGSSMTIg)\n",
        "\n",
        "La manera mas facil de obtener el audio de ejemplo es con yt-dlp y ffmpeg. pero en este caso esta bloqueado detras de colab.\n",
        "\n",
        "```sh\n",
        "!yt-dlp -x --audio-format mp3 -o \"audio.mp3\" https://www.youtube.com/watch?v=g8X6Qix-QUA\n",
        "\n",
        "```\n",
        "\n",
        "por ahora vamos a obtenerlo ya convertido en audio desde nuestros buckets."
      ],
      "metadata": {
        "id": "PaPlirx0TafB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "audio_url = \"https://storage.googleapis.com/tallerdp_publico/audio01.mp3\"\n",
        "\n",
        "response = requests.get(audio_url)\n",
        "\n",
        "with open(\"audio01.mp3\", \"wb\") as f:\n",
        "    f.write(response.content)"
      ],
      "metadata": {
        "id": "K_BrhlePTl9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcripcion\n",
        "\n",
        "Lo segundo que tenemos que hacer es decidir que tamaño de whisper queremos usar, hay que tener en cuenta que las versiones mas pequeñas de whisper no son muy buenas haciendo transcripciones en español.\n",
        "\n",
        "Hasta hace algunos meses la unica opcion viable para mi era Whisper Large, pero con la reciente introduccion de Whisper Turbo ya podemos elegir entre velocidad y fiabilidad de la transcripcion.\n",
        "\n",
        "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |\n",
        "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |\n",
        "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |\n",
        "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |\n",
        "\n",
        "\n",
        "Para nuestros ejemplos vamos a usar whisper turbo"
      ],
      "metadata": {
        "id": "bIS2oiQIWw7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "\n",
        "# tenemos tarjeta de video? esto es muy importante cuando trabajos con whisper\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "#podemos usar un modelo mas pequeño, si no tenemos problemas con el idioma\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "#job de audio->texto\n",
        "whisper_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\", #whisper tiene mas de 1 tarea\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        "    chunk_length_s=30, # bajar numero para reducir uso de memoria\n",
        "    batch_size=16 #\n",
        ")\n",
        "\n",
        "#procesamos el mp3 y sacamos los timestamps, forzamos la deteccion a español por ahora\n",
        "result = whisper_pipeline(\"audio01.mp3\", generate_kwargs={\"language\": \"spanish\"},\n",
        "                          return_timestamps=True)\n",
        "\n",
        "\n",
        "#imprimimos el resultado, aqui deberiamos de almacenarlo en algun lugar.\n",
        "print(result)"
      ],
      "metadata": {
        "id": "E1u4HqG7XcPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contexto\n",
        "\n",
        "Con que tanta informacion estamos trabajando?  y de que manera podemos trabajr con ella?"
      ],
      "metadata": {
        "id": "OAsLVFANehEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokens = encoding.encode(result['text'])\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "7BmoS2FReeQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es un numero de tokens razonable para poder hacer extraccion de entidades usando alguna variante de [BERT](https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-ner). Pero para no tener algun problema podemos trabajar con los chunks, o pedazos de transcripcion que creo whisper."
      ],
      "metadata": {
        "id": "D-tlNA5yfiwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#usamos Bert para sacar entidades usando NER\n",
        "ner_pipeline = pipeline(task=\"ner\",\n",
        "                        model=\"mrm8488/bert-spanish-cased-finetuned-ner\",\n",
        "                        device=device,\n",
        "                        aggregation_strategy=\"simple\")\n",
        "\n",
        "\n",
        "all_entities = []\n",
        "for chunk in result[\"chunks\"]:\n",
        "    ner = ner_pipeline(chunk[\"text\"])\n",
        "    entities = [\n",
        "                {\n",
        "                    \"type\": pred[\"entity_group\"],\n",
        "                    \"score\": round(pred[\"score\"], 4),\n",
        "                    \"word\": pred[\"word\"],\n",
        "                    \"start\": pred[\"start\"],\n",
        "                    \"end\": pred[\"end\"],\n",
        "                }\n",
        "                for pred in ner\n",
        "            ]\n",
        "    for entity in entities:\n",
        "      all_entities.append(entity)\n",
        "\n",
        "df = pd.DataFrame(all_entities)\n",
        "df"
      ],
      "metadata": {
        "id": "SppWVHCbfo3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todas las personas y ubicaciones unidas\n",
        "df_filtered = df[df[\"type\"].isin([\"PER\", \"LOC\"])]\n",
        "df_unique = df_filtered.drop_duplicates(subset=[\"type\", \"word\"])\n",
        "\n",
        "df_unique"
      ],
      "metadata": {
        "id": "bmAjdkiLinR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraccion usando LLMs\n",
        "\n",
        "\n",
        "Tambien podemos generar extracciones usando modelos de lenguaje, aqui ya podemos extraer informacion mas compleja.\n",
        "\n"
      ],
      "metadata": {
        "id": "rSUS8QNejRXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('GOOGLE_API_KEY'),\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "prompt = f\"\"\"\n",
        "De la siguiente transcripcion de un audio:\n",
        "\n",
        "```\n",
        "{result['text']}\n",
        "```\n",
        "\n",
        "Extrae los siguientes datos:\n",
        "- Nombre del juez (obligatorio)\n",
        "-  Lista de personas en la audiencia (obligatorio), donde cada persona incluye:\n",
        " - Nombre (opcional)\n",
        " - Cargo (opcional)\n",
        "- Resumen de toda la audiencia (obligatorio)\n",
        "- Tipo de audiencia (opcional)\n",
        "- Ubicación de la audiencia (opcional)\n",
        "- Sentencia de la audiencia (opcional)\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gemini-2.5-flash\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Eres un sistema de extracción de información de transcripciones de audio, solo generas JSON valido de acuerdo con el schema proporcionado.\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": prompt\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ],\n",
        "  response_format={\n",
        "    \"type\": \"json_schema\",\n",
        "    \"json_schema\": {\n",
        "      \"name\": \"audience\",\n",
        "      \"strict\": True,\n",
        "      \"schema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"judge_name\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Nombre del juez que preside la audiencia.\"\n",
        "          },\n",
        "          \"participants\": {\n",
        "            \"type\": \"array\",\n",
        "            \"description\": \"Lista de personas en la audiencia.\",\n",
        "            \"items\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                \"name\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"Nombre de la persona.\"\n",
        "                },\n",
        "                \"position\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"Cargo de la persona en la audiencia.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\n",
        "                \"name\",\n",
        "                \"position\"\n",
        "              ],\n",
        "              \"additionalProperties\": False\n",
        "            }\n",
        "          },\n",
        "          \"audience_summary\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Resumen de toda la audiencia.\"\n",
        "          },\n",
        "          \"audience_type\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Tipo de audiencia.\"\n",
        "          },\n",
        "          \"audience_location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Ubicación donde se lleva a cabo la audiencia.\"\n",
        "          },\n",
        "          \"audience_sentence\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"Sentencia resultante de la audiencia.\"\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\n",
        "          \"judge_name\",\n",
        "          \"participants\",\n",
        "          \"audience_summary\",\n",
        "          \"audience_type\",\n",
        "          \"audience_location\",\n",
        "          \"audience_sentence\"\n",
        "        ],\n",
        "        \"additionalProperties\": False\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  temperature=0.1\n",
        ")\n",
        "\n",
        "print(response.choices[0].message)"
      ],
      "metadata": {
        "id": "DQN61DwllDbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = json.loads(response.choices[0].message.content)\n",
        "\n",
        "print(json.dumps(data, indent=4))\n"
      ],
      "metadata": {
        "id": "yv_KZEVSmh4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini\n",
        "\n",
        "Antes de Gemini 2.0 flash esta era la manera indeal de extraer informacion de audio, pero ahora todo este proceso es mucho mas rapido y barato usando directamente Gemini sin pasos intermedios.\n",
        "\n",
        "Hay casos en los que todavia se recomienda usar las opciones anteriores."
      ],
      "metadata": {
        "id": "7G0m_oqTnHDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    model = \"gemini-2.5-flash\"\n",
        "    audio_file = client.files.upload(file=\"audio01.mp3\")\n",
        "    prompt = \"\"\"Del audio adjuntado  extrae los siguientes datos:\n",
        "\n",
        "- Nombre del juez (obligatorio)\n",
        "-  Lista de personas en la audiencia (obligatorio), donde cada persona incluye:\n",
        " - Nombre (opcional)\n",
        " - Cargo (opcional)\n",
        "- Estado de animo / Sentimiento (opcional)\n",
        "- Resumen de toda la audiencia (obligatorio)\n",
        "- Tipo de audiencia (opcional)\n",
        "- Calidad del audio (opcional)\n",
        "- Ubicación de la audiencia (opcional)\n",
        "- Sentencia de la audiencia (opcional)\"\"\"\n",
        "\n",
        "    contents=[prompt, audio_file]\n",
        "\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=genai.types.Schema(\n",
        "            type = genai.types.Type.OBJECT,\n",
        "            properties = {\n",
        "                \"nombre_juez\": genai.types.Schema(\n",
        "                    type = genai.types.Type.STRING,\n",
        "                ),\n",
        "                \"participantes\": genai.types.Schema(\n",
        "                    type = genai.types.Type.ARRAY,\n",
        "                    items = genai.types.Schema(\n",
        "                        type = genai.types.Type.OBJECT,\n",
        "                        properties = {\n",
        "                            \"nombre\": genai.types.Schema(\n",
        "                                type = genai.types.Type.STRING,\n",
        "                            ),\n",
        "                            \"cargp\": genai.types.Schema(\n",
        "                                type = genai.types.Type.STRING,\n",
        "                            ),\n",
        "                            \"sentimiento\": genai.types.Schema(\n",
        "                                type = genai.types.Type.STRING,\n",
        "                            ),\n",
        "                        },\n",
        "                    ),\n",
        "                ),\n",
        "                \"resumen\": genai.types.Schema(\n",
        "                    type = genai.types.Type.STRING,\n",
        "                ),\n",
        "                \"tipo_audiencia\": genai.types.Schema(\n",
        "                    type = genai.types.Type.STRING,\n",
        "                ),\n",
        "                \"calidad_audio\": genai.types.Schema(\n",
        "                    type = genai.types.Type.STRING,\n",
        "                ),\n",
        "                \"ubicacio_audiencia\": genai.types.Schema(\n",
        "                    type = genai.types.Type.STRING,\n",
        "                ),\n",
        "            },\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\"\")\n",
        "\n",
        "generate()"
      ],
      "metadata": {
        "id": "pMj7fBgrpPGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio\n",
        "\n",
        "- Que tantas veces se dijo por favor?\n",
        "- Cuantas muletillas vocales se usaron en la audiencia?  "
      ],
      "metadata": {
        "id": "JDseOpAHqhvF"
      }
    }
  ]
}